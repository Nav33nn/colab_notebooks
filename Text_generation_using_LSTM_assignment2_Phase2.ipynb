{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation using LSTM",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBg9N5m_Uz4C",
        "colab_type": "code",
        "outputId": "7da8c5e2-e247-466e-bbab-9a55e867e061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://www.gutenberg.org/files/11/11-0.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 16:25:42--  https://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173595 (170K) [text/plain]\n",
            "Saving to: ‘11-0.txt’\n",
            "\n",
            "\r11-0.txt              0%[                    ]       0  --.-KB/s               \r11-0.txt            100%[===================>] 169.53K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-07-26 16:25:43 (1.14 MB/s) - ‘11-0.txt’ saved [173595/173595]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-RejPVLVAnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('11-0.txt').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BbVPwv8VXl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.split(\"*** START OF THIS PROJECT GUTENBERG EBOOK ALICE’S ADVENTURES IN WONDERLAND ***\")[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52iGrVkwWzqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.split(\"THE END\")[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa2PTL5Z-Ajq",
        "colab_type": "code",
        "outputId": "2770eb43-f19a-4745-d33d-aa6e28edc158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "\n",
        "\n",
        "# load ascii text and covert to lowercase and remove punctuations\n",
        "raw_text = data.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 500\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "X = pad_sequences(X)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  139103\n",
            "Total Vocab:  34\n",
            "Total Patterns:  138603\n",
            "Epoch 1/100\n",
            "138603/138603 [==============================] - 361s 3ms/step - loss: 2.9521\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.95215, saving model to weights-improvement-01-2.9521-bigger.hdf5\n",
            "Epoch 2/100\n",
            "138603/138603 [==============================] - 354s 3ms/step - loss: 2.9344\n",
            "\n",
            "Epoch 00002: loss improved from 2.95215 to 2.93437, saving model to weights-improvement-02-2.9344-bigger.hdf5\n",
            "Epoch 3/100\n",
            "138603/138603 [==============================] - 352s 3ms/step - loss: 2.9337\n",
            "\n",
            "Epoch 00003: loss improved from 2.93437 to 2.93368, saving model to weights-improvement-03-2.9337-bigger.hdf5\n",
            "Epoch 4/100\n",
            "138603/138603 [==============================] - 352s 3ms/step - loss: 2.9330\n",
            "\n",
            "Epoch 00004: loss improved from 2.93368 to 2.93305, saving model to weights-improvement-04-2.9330-bigger.hdf5\n",
            "Epoch 5/100\n",
            "138603/138603 [==============================] - 352s 3ms/step - loss: 2.9325\n",
            "\n",
            "Epoch 00005: loss improved from 2.93305 to 2.93246, saving model to weights-improvement-05-2.9325-bigger.hdf5\n",
            "Epoch 6/100\n",
            "138603/138603 [==============================] - 351s 3ms/step - loss: 2.9323\n",
            "\n",
            "Epoch 00006: loss improved from 2.93246 to 2.93230, saving model to weights-improvement-06-2.9323-bigger.hdf5\n",
            "Epoch 7/100\n",
            "138603/138603 [==============================] - 352s 3ms/step - loss: 2.9323\n",
            "\n",
            "Epoch 00007: loss did not improve from 2.93230\n",
            "Epoch 8/100\n",
            "138603/138603 [==============================] - 351s 3ms/step - loss: 2.9324\n",
            "\n",
            "Epoch 00008: loss did not improve from 2.93230\n",
            "Epoch 9/100\n",
            "138603/138603 [==============================] - 351s 3ms/step - loss: 2.9320\n",
            "\n",
            "Epoch 00009: loss improved from 2.93230 to 2.93197, saving model to weights-improvement-09-2.9320-bigger.hdf5\n",
            "Epoch 10/100\n",
            "138603/138603 [==============================] - 351s 3ms/step - loss: 2.9319\n",
            "\n",
            "Epoch 00010: loss improved from 2.93197 to 2.93190, saving model to weights-improvement-10-2.9319-bigger.hdf5\n",
            "Epoch 11/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9319\n",
            "\n",
            "Epoch 00011: loss improved from 2.93190 to 2.93189, saving model to weights-improvement-11-2.9319-bigger.hdf5\n",
            "Epoch 12/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9319\n",
            "\n",
            "Epoch 00012: loss improved from 2.93189 to 2.93187, saving model to weights-improvement-12-2.9319-bigger.hdf5\n",
            "Epoch 13/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9318\n",
            "\n",
            "Epoch 00013: loss improved from 2.93187 to 2.93178, saving model to weights-improvement-13-2.9318-bigger.hdf5\n",
            "Epoch 14/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9318\n",
            "\n",
            "Epoch 00014: loss did not improve from 2.93178\n",
            "Epoch 15/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9315\n",
            "\n",
            "Epoch 00015: loss improved from 2.93178 to 2.93150, saving model to weights-improvement-15-2.9315-bigger.hdf5\n",
            "Epoch 16/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9317\n",
            "\n",
            "Epoch 00016: loss did not improve from 2.93150\n",
            "Epoch 17/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9313\n",
            "\n",
            "Epoch 00017: loss improved from 2.93150 to 2.93126, saving model to weights-improvement-17-2.9313-bigger.hdf5\n",
            "Epoch 18/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9316\n",
            "\n",
            "Epoch 00018: loss did not improve from 2.93126\n",
            "Epoch 19/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9315\n",
            "\n",
            "Epoch 00019: loss did not improve from 2.93126\n",
            "Epoch 20/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9313\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.93126\n",
            "Epoch 21/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9314\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.93126\n",
            "Epoch 22/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9314\n",
            "\n",
            "Epoch 00022: loss did not improve from 2.93126\n",
            "Epoch 23/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9312\n",
            "\n",
            "Epoch 00023: loss improved from 2.93126 to 2.93120, saving model to weights-improvement-23-2.9312-bigger.hdf5\n",
            "Epoch 24/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9315\n",
            "\n",
            "Epoch 00024: loss did not improve from 2.93120\n",
            "Epoch 25/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9312\n",
            "\n",
            "Epoch 00025: loss improved from 2.93120 to 2.93116, saving model to weights-improvement-25-2.9312-bigger.hdf5\n",
            "Epoch 26/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9312\n",
            "\n",
            "Epoch 00026: loss improved from 2.93116 to 2.93116, saving model to weights-improvement-26-2.9312-bigger.hdf5\n",
            "Epoch 27/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00027: loss improved from 2.93116 to 2.93111, saving model to weights-improvement-27-2.9311-bigger.hdf5\n",
            "Epoch 28/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.93111\n",
            "Epoch 29/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00029: loss improved from 2.93111 to 2.93104, saving model to weights-improvement-29-2.9310-bigger.hdf5\n",
            "Epoch 30/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.93104\n",
            "Epoch 31/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00031: loss did not improve from 2.93104\n",
            "Epoch 32/100\n",
            "138603/138603 [==============================] - 348s 3ms/step - loss: 2.9312\n",
            "\n",
            "Epoch 00032: loss did not improve from 2.93104\n",
            "Epoch 33/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00033: loss improved from 2.93104 to 2.93098, saving model to weights-improvement-33-2.9310-bigger.hdf5\n",
            "Epoch 34/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00034: loss did not improve from 2.93098\n",
            "Epoch 35/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00035: loss did not improve from 2.93098\n",
            "Epoch 36/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00036: loss improved from 2.93098 to 2.93097, saving model to weights-improvement-36-2.9310-bigger.hdf5\n",
            "Epoch 37/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00037: loss did not improve from 2.93097\n",
            "Epoch 38/100\n",
            "138603/138603 [==============================] - 348s 3ms/step - loss: 2.9312\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.93097\n",
            "Epoch 39/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00039: loss improved from 2.93097 to 2.93097, saving model to weights-improvement-39-2.9310-bigger.hdf5\n",
            "Epoch 40/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00040: loss did not improve from 2.93097\n",
            "Epoch 41/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00041: loss did not improve from 2.93097\n",
            "Epoch 42/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00042: loss did not improve from 2.93097\n",
            "Epoch 43/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00043: loss improved from 2.93097 to 2.93095, saving model to weights-improvement-43-2.9310-bigger.hdf5\n",
            "Epoch 44/100\n",
            "138603/138603 [==============================] - 347s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00044: loss improved from 2.93095 to 2.93077, saving model to weights-improvement-44-2.9308-bigger.hdf5\n",
            "Epoch 45/100\n",
            "138603/138603 [==============================] - 348s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00045: loss did not improve from 2.93077\n",
            "Epoch 46/100\n",
            "138603/138603 [==============================] - 348s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00046: loss did not improve from 2.93077\n",
            "Epoch 47/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00047: loss did not improve from 2.93077\n",
            "Epoch 48/100\n",
            "138603/138603 [==============================] - 349s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.93077\n",
            "Epoch 49/100\n",
            "138603/138603 [==============================] - 350s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00049: loss did not improve from 2.93077\n",
            "Epoch 50/100\n",
            "138603/138603 [==============================] - 351s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00050: loss did not improve from 2.93077\n",
            "Epoch 51/100\n",
            "  3584/138603 [..............................] - ETA: 5:39 - loss: 2.9199"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c1c8db549dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8P1SMSM_8nB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3b2ebe95-9a3c-4686-bef4-c41204998764"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clY_3OOOE4_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7383b166-f3ef-4c28-8c71-b1d69a6d0569"
      },
      "source": [
        "!cp weights-improvement-44-2.9308-bigger.hdf5 /content/gdrive/My Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger.hdf5"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target 'Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger.hdf5' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5977wPXFGrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbfd67ab-335e-486d-e58a-bb3a6037055f"
      },
      "source": [
        "from shutil import copyfile\n",
        "copyfile(\"weights-improvement-44-2.9308-bigger.hdf5\", \"/content/gdrive/My Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger.hdf5\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger.hdf5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw7d1OyeFnOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(\"/content/gdrive/My Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger_weights.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzpJTEONGZSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c74ded45-f260-49d1-9b1b-0fba82408b90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI3g0iqOFzsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94039907-8e9c-4d83-d8ce-38360e618467"
      },
      "source": [
        "# Larger LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "\n",
        "\n",
        "# load ascii text and covert to lowercase and remove punctuations\n",
        "raw_text = data.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 500\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "X = pad_sequences(X)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "model = load_model('/content/gdrive/My Drive/Colab Notebooks/text_gen_weights-improvement-44-2.9308-bigger.hdf5')\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=50, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  139103\n",
            "Total Vocab:  34\n",
            "Total Patterns:  138603\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 16:31:18.498258 140412088608640 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0726 16:31:18.500167 140412088608640 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0726 16:31:18.512406 140412088608640 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0726 16:31:18.529063 140412088608640 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0726 16:31:19.129880 140412088608640 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0726 16:31:22.144903 140412088608640 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "138603/138603 [==============================] - 353s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.93082, saving model to weights-improvement-01-2.9308-bigger.hdf5\n",
            "Epoch 2/50\n",
            "138603/138603 [==============================] - 353s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00002: loss did not improve from 2.93082\n",
            "Epoch 3/50\n",
            "138603/138603 [==============================] - 355s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00003: loss did not improve from 2.93082\n",
            "Epoch 4/50\n",
            "138603/138603 [==============================] - 356s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00004: loss did not improve from 2.93082\n",
            "Epoch 5/50\n",
            "138603/138603 [==============================] - 356s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00005: loss did not improve from 2.93082\n",
            "Epoch 6/50\n",
            "138603/138603 [==============================] - 358s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00006: loss did not improve from 2.93082\n",
            "Epoch 7/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00007: loss did not improve from 2.93082\n",
            "Epoch 8/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00008: loss improved from 2.93082 to 2.93075, saving model to weights-improvement-08-2.9308-bigger.hdf5\n",
            "Epoch 9/50\n",
            "138603/138603 [==============================] - 361s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00009: loss did not improve from 2.93075\n",
            "Epoch 10/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00010: loss did not improve from 2.93075\n",
            "Epoch 11/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00011: loss did not improve from 2.93075\n",
            "Epoch 12/50\n",
            "138603/138603 [==============================] - 360s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00012: loss did not improve from 2.93075\n",
            "Epoch 13/50\n",
            "138603/138603 [==============================] - 362s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00013: loss did not improve from 2.93075\n",
            "Epoch 14/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00014: loss did not improve from 2.93075\n",
            "Epoch 15/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00015: loss did not improve from 2.93075\n",
            "Epoch 16/50\n",
            "138603/138603 [==============================] - 356s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00016: loss did not improve from 2.93075\n",
            "Epoch 17/50\n",
            "138603/138603 [==============================] - 360s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00017: loss improved from 2.93075 to 2.93069, saving model to weights-improvement-17-2.9307-bigger.hdf5\n",
            "Epoch 18/50\n",
            "138603/138603 [==============================] - 361s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00018: loss did not improve from 2.93069\n",
            "Epoch 19/50\n",
            "138603/138603 [==============================] - 361s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00019: loss did not improve from 2.93069\n",
            "Epoch 20/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9325\n",
            "\n",
            "Epoch 00020: loss did not improve from 2.93069\n",
            "Epoch 21/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9321\n",
            "\n",
            "Epoch 00021: loss did not improve from 2.93069\n",
            "Epoch 22/50\n",
            "138603/138603 [==============================] - 355s 3ms/step - loss: 2.9314\n",
            "\n",
            "Epoch 00022: loss did not improve from 2.93069\n",
            "Epoch 23/50\n",
            "138603/138603 [==============================] - 354s 3ms/step - loss: 2.9313\n",
            "\n",
            "Epoch 00023: loss did not improve from 2.93069\n",
            "Epoch 24/50\n",
            "138603/138603 [==============================] - 356s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00024: loss did not improve from 2.93069\n",
            "Epoch 25/50\n",
            "138603/138603 [==============================] - 358s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00025: loss did not improve from 2.93069\n",
            "Epoch 26/50\n",
            "138603/138603 [==============================] - 355s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00026: loss did not improve from 2.93069\n",
            "Epoch 27/50\n",
            "138603/138603 [==============================] - 354s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00027: loss did not improve from 2.93069\n",
            "Epoch 28/50\n",
            "138603/138603 [==============================] - 354s 3ms/step - loss: 2.9311\n",
            "\n",
            "Epoch 00028: loss did not improve from 2.93069\n",
            "Epoch 29/50\n",
            "138603/138603 [==============================] - 355s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00029: loss did not improve from 2.93069\n",
            "Epoch 30/50\n",
            "138603/138603 [==============================] - 355s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00030: loss did not improve from 2.93069\n",
            "Epoch 31/50\n",
            "138603/138603 [==============================] - 354s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00031: loss did not improve from 2.93069\n",
            "Epoch 32/50\n",
            "138603/138603 [==============================] - 358s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00032: loss did not improve from 2.93069\n",
            "Epoch 33/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00033: loss did not improve from 2.93069\n",
            "Epoch 34/50\n",
            "138603/138603 [==============================] - 367s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00034: loss did not improve from 2.93069\n",
            "Epoch 35/50\n",
            "138603/138603 [==============================] - 366s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00035: loss did not improve from 2.93069\n",
            "Epoch 36/50\n",
            "138603/138603 [==============================] - 370s 3ms/step - loss: 2.9309\n",
            "\n",
            "Epoch 00036: loss did not improve from 2.93069\n",
            "Epoch 37/50\n",
            "138603/138603 [==============================] - 365s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00037: loss did not improve from 2.93069\n",
            "Epoch 38/50\n",
            "138603/138603 [==============================] - 366s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00038: loss did not improve from 2.93069\n",
            "Epoch 39/50\n",
            "138603/138603 [==============================] - 377s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00039: loss did not improve from 2.93069\n",
            "Epoch 40/50\n",
            "138603/138603 [==============================] - 372s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00040: loss did not improve from 2.93069\n",
            "Epoch 41/50\n",
            "138603/138603 [==============================] - 375s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00041: loss did not improve from 2.93069\n",
            "Epoch 42/50\n",
            "138603/138603 [==============================] - 372s 3ms/step - loss: 2.9310\n",
            "\n",
            "Epoch 00042: loss did not improve from 2.93069\n",
            "Epoch 43/50\n",
            "138603/138603 [==============================] - 373s 3ms/step - loss: 2.9306\n",
            "\n",
            "Epoch 00043: loss improved from 2.93069 to 2.93056, saving model to weights-improvement-43-2.9306-bigger.hdf5\n",
            "Epoch 44/50\n",
            "138603/138603 [==============================] - 365s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00044: loss did not improve from 2.93056\n",
            "Epoch 45/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9308\n",
            "\n",
            "Epoch 00045: loss did not improve from 2.93056\n",
            "Epoch 46/50\n",
            "138603/138603 [==============================] - 360s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00046: loss did not improve from 2.93056\n",
            "Epoch 47/50\n",
            "138603/138603 [==============================] - 362s 3ms/step - loss: 2.9307\n",
            "\n",
            "Epoch 00047: loss did not improve from 2.93056\n",
            "Epoch 48/50\n",
            "138603/138603 [==============================] - 359s 3ms/step - loss: 2.9306\n",
            "\n",
            "Epoch 00048: loss did not improve from 2.93056\n",
            "Epoch 49/50\n",
            "138603/138603 [==============================] - 358s 3ms/step - loss: 2.9306\n",
            "\n",
            "Epoch 00049: loss did not improve from 2.93056\n",
            "Epoch 50/50\n",
            "138603/138603 [==============================] - 360s 3ms/step - loss: 2.9306\n",
            "\n",
            "Epoch 00050: loss did not improve from 2.93056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb3ef22aa58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOPYFm8sKGRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "4f165938-875d-4d61-c6df-a5d3eccbcc46"
      },
      "source": [
        "# Load LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "import glob\n",
        "import os\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "import string\n",
        "# load ascii text and covert to lowercase\n",
        "# filename = \"wonderland.txt\"\n",
        "# raw_text = open(filename).read()\n",
        "raw_text = data\n",
        "raw_text = data.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 500\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
        "# model.load_weights(filename)\n",
        "model = load_model(max(glob.glob('*'), key=os.path.getctime))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  139103\n",
            "Total Vocab:  34\n",
            "Total Patterns:  138603\n",
            "Seed:\n",
            "\" f and that in\n",
            "about half no time take your choice’\n",
            "\n",
            "the duchess took her choice and was gone in a moment\n",
            "\n",
            "‘let’s go on with the game’ the queen said to alice and alice was\n",
            "too much frightened to say a word but slowly followed her back to the\n",
            "croquetground\n",
            "\n",
            "the other guests had taken advantage of the queen’s absence and were\n",
            "resting in the shade however the moment they saw her they hurried\n",
            "back to the game the queen merely remarking that a moment’s delay would\n",
            "cost them their lives\n",
            "\n",
            "all the time  \"\n",
            "                                                                                                                                                                                                                                                                                "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8b2977c7e04d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5mxFQSfODxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}